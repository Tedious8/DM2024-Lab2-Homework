{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 陳培熹\n",
    "\n",
    "Student ID: 113065425\n",
    "\n",
    "GitHub ID: Tedious8\n",
    "\n",
    "Kaggle name: Tadeus\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Scoreboard snapshot](./img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISCLAIMER: THIS CODE WAS RUNNING ON KAGGLE NOTEBOOK WITH GPU P100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:05:26.834731Z",
     "iopub.status.busy": "2024-11-24T14:05:26.834057Z",
     "iopub.status.idle": "2024-11-24T14:05:27.134378Z",
     "shell.execute_reply": "2024-11-24T14:05:27.133677Z",
     "shell.execute_reply.started": "2024-11-24T14:05:26.834698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import TFAutoModelForSequenceClassification, DataCollatorWithPadding, AutoTokenizer, AutoConfig, create_optimizer\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed to part 2.2 if the data has been preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocess the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "data_identification = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv')\n",
    "emotion = pd.read_csv('/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv')\n",
    "with open('/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json', 'r') as file:\n",
    "    tweets_DM = []\n",
    "    for line in tqdm(file):\n",
    "        # Parse each line as a JSON object\n",
    "        json_object = json.loads(line)\n",
    "        tweets_DM.append(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "df = pd.merge(data_identification, emotion, on='tweet_id', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tweet information\n",
    "tweets_df = []\n",
    "for tweet in tqdm(tweets_DM):\n",
    "    tweet_info = tweet['_source']['tweet']\n",
    "    tweet_info['_score'] = tweet['_score']\n",
    "    tweet_info['_index'] = tweet['_index']\n",
    "    tweet_info['_crawldate'] = tweet['_crawldate']\n",
    "    tweet_info['_type'] = tweet['_type']\n",
    "    tweets_df.append(tweet_info)\n",
    "\n",
    "# Convert to DataFrame and drop unnecessary columns\n",
    "tweets_df = pd.DataFrame(tweets_df)\n",
    "tweets_df.drop(columns=['_index', '_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tweet information into the main dataframe\n",
    "df = pd.merge(df, tweets_df, on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into training and testing datasets\n",
    "raw_train = df[df['identification'] == 'train'][['tweet_id', 'emotion', 'text']]\n",
    "raw_test = df[df['identification'] == 'test'][['tweet_id', 'emotion', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets to pickles\n",
    "raw_train.to_pickle('raw_train.pkl')\n",
    "raw_test.to_pickle('raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:05:29.582958Z",
     "iopub.status.busy": "2024-11-24T14:05:29.582007Z",
     "iopub.status.idle": "2024-11-24T14:05:42.793466Z",
     "shell.execute_reply": "2024-11-24T14:05:42.792741Z",
     "shell.execute_reply.started": "2024-11-24T14:05:29.582925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load preprocessed datasets\n",
    "raw_train = pd.read_pickle('/kaggle/input/dm2024-lab2-homework-data/raw_train.pkl')\n",
    "raw_test = pd.read_pickle('/kaggle/input/dm2024-lab2-homework-data/raw_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training using Cardiffnlp model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the direct model from the Cardiffnlp if it's the first time training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:06:03.065596Z",
     "iopub.status.busy": "2024-11-24T14:06:03.065092Z",
     "iopub.status.idle": "2024-11-24T14:06:03.069823Z",
     "shell.execute_reply": "2024-11-24T14:06:03.068708Z",
     "shell.execute_reply.started": "2024-11-24T14:06:03.065568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Specify the pre-trained model\n",
    "MODEL = 'cardiffnlp/twitter-roberta-base-sentiment-latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle model path\n",
    "MODEL = '/kaggle/input/dm2024-lab2-homework/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:06:03.073262Z",
     "iopub.status.busy": "2024-11-24T14:06:03.072896Z",
     "iopub.status.idle": "2024-11-24T14:06:03.735252Z",
     "shell.execute_reply": "2024-11-24T14:06:03.734385Z",
     "shell.execute_reply.started": "2024-11-24T14:06:03.073220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4a10a882384fb282444657757e8c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57249deeaf9949098be65a46ca86be54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0698db3f4b60441390474c53308f301c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:06:03.736575Z",
     "iopub.status.busy": "2024-11-24T14:06:03.736292Z",
     "iopub.status.idle": "2024-11-24T14:06:03.796454Z",
     "shell.execute_reply": "2024-11-24T14:06:03.795631Z",
     "shell.execute_reply.started": "2024-11-24T14:06:03.736544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode emotion labels\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.array(raw_train.emotion).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:06:03.798060Z",
     "iopub.status.busy": "2024-11-24T14:06:03.797626Z",
     "iopub.status.idle": "2024-11-24T14:06:03.806242Z",
     "shell.execute_reply": "2024-11-24T14:06:03.805501Z",
     "shell.execute_reply.started": "2024-11-24T14:06:03.798024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define preprocessing function for tokenization and encoding labels\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize text with truncation\n",
    "    out = tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "    # Encode labels and get class indices\n",
    "    out['label'] = encoder.transform(np.array(examples['emotion']).reshape(-1, 1)).toarray().argmax(axis=1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:06:05.439849Z",
     "iopub.status.busy": "2024-11-24T14:06:05.439189Z",
     "iopub.status.idle": "2024-11-24T14:07:24.118489Z",
     "shell.execute_reply": "2024-11-24T14:07:24.117574Z",
     "shell.execute_reply.started": "2024-11-24T14:06:05.439810Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24716dc722534840bbf123f6f9ae985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1455563 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Hugging Face Dataset and tokenize it\n",
    "ds = Dataset.from_pandas(raw_train[['text', 'emotion']])\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:24.127334Z",
     "iopub.status.busy": "2024-11-24T14:07:24.126793Z",
     "iopub.status.idle": "2024-11-24T14:07:24.459350Z",
     "shell.execute_reply": "2024-11-24T14:07:24.458450Z",
     "shell.execute_reply.started": "2024-11-24T14:07:24.127296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and validation sets\n",
    "tokenized_ds = tokenized_ds.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:24.119768Z",
     "iopub.status.busy": "2024-11-24T14:07:24.119529Z",
     "iopub.status.idle": "2024-11-24T14:07:24.123985Z",
     "shell.execute_reply": "2024-11-24T14:07:24.123106Z",
     "shell.execute_reply.started": "2024-11-24T14:07:24.119745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:24.461181Z",
     "iopub.status.busy": "2024-11-24T14:07:24.460536Z",
     "iopub.status.idle": "2024-11-24T14:07:35.918462Z",
     "shell.execute_reply": "2024-11-24T14:07:35.917758Z",
     "shell.execute_reply.started": "2024-11-24T14:07:24.461141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa4ad2b745e4793a1876894b718f92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install and import evaluation metric\n",
    "%pip install evaluate -q\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:35.919871Z",
     "iopub.status.busy": "2024-11-24T14:07:35.919588Z",
     "iopub.status.idle": "2024-11-24T14:07:35.925500Z",
     "shell.execute_reply": "2024-11-24T14:07:35.923806Z",
     "shell.execute_reply.started": "2024-11-24T14:07:35.919842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define metric computation function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert logits to predicted class indices\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:35.927102Z",
     "iopub.status.busy": "2024-11-24T14:07:35.926734Z",
     "iopub.status.idle": "2024-11-24T14:07:35.942825Z",
     "shell.execute_reply": "2024-11-24T14:07:35.941997Z",
     "shell.execute_reply.started": "2024-11-24T14:07:35.927074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5, 'surprise': 6, 'trust': 7}\n",
      "id2label: {0: 'anger', 1: 'anticipation', 2: 'disgust', 3: 'fear', 4: 'joy', 5: 'sadness', 6: 'surprise', 7: 'trust'}\n"
     ]
    }
   ],
   "source": [
    "# Create label mappings for the model\n",
    "categories = encoder.categories_[0]  # Get the list of unique categories\n",
    "label2id = {label: idx for idx, label in enumerate(categories)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I don't directly utilize 5 epochs for training. Instead I do one epoch for three different times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:35.944115Z",
     "iopub.status.busy": "2024-11-24T14:07:35.943870Z",
     "iopub.status.idle": "2024-11-24T14:07:38.158416Z",
     "shell.execute_reply": "2024-11-24T14:07:38.157662Z",
     "shell.execute_reply.started": "2024-11-24T14:07:35.944091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_ds[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below two cells is the setup for first time training and can be skipped if it's not the first time training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:38.159853Z",
     "iopub.status.busy": "2024-11-24T14:07:38.159571Z",
     "iopub.status.idle": "2024-11-24T14:07:38.214129Z",
     "shell.execute_reply": "2024-11-24T14:07:38.213539Z",
     "shell.execute_reply.started": "2024-11-24T14:07:38.159826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model configuration and set label mappings\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "config.num_labels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:38.215265Z",
     "iopub.status.busy": "2024-11-24T14:07:38.215052Z",
     "iopub.status.idle": "2024-11-24T14:07:40.401085Z",
     "shell.execute_reply": "2024-11-24T14:07:40.400252Z",
     "shell.execute_reply.started": "2024-11-24T14:07:38.215244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c63a69c8c749408f7895b50b6a3464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape (2, 768) in the checkpoint and (768, 8) in the model instantiated\n",
      "- classifier.bias: found shape (2,) in the checkpoint and (8,) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL, config=config, ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is only used after the saved model exist (first training) on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:40.402636Z",
     "iopub.status.busy": "2024-11-24T14:07:40.402299Z",
     "iopub.status.idle": "2024-11-24T14:07:40.791979Z",
     "shell.execute_reply": "2024-11-24T14:07:40.791307Z",
     "shell.execute_reply.started": "2024-11-24T14:07:40.402595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare datasets for TensorFlow\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:40.793439Z",
     "iopub.status.busy": "2024-11-24T14:07:40.793084Z",
     "iopub.status.idle": "2024-11-24T14:07:40.806702Z",
     "shell.execute_reply": "2024-11-24T14:07:40.806044Z",
     "shell.execute_reply.started": "2024-11-24T14:07:40.793398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:40.823101Z",
     "iopub.status.busy": "2024-11-24T14:07:40.822864Z",
     "iopub.status.idle": "2024-11-24T14:07:40.828073Z",
     "shell.execute_reply": "2024-11-24T14:07:40.827199Z",
     "shell.execute_reply.started": "2024-11-24T14:07:40.823077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add metric evaluation callback\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)                            \n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T14:07:40.829407Z",
     "iopub.status.busy": "2024-11-24T14:07:40.829054Z",
     "iopub.status.idle": "2024-11-24T16:23:57.636193Z",
     "shell.execute_reply": "2024-11-24T16:23:57.635342Z",
     "shell.execute_reply.started": "2024-11-24T14:07:40.829371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function infer_framework at 0x7f20dddb2f80> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732457295.188817     112 service.cc:145] XLA service 0x7f1edf93de70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732457295.188872     112 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1732457295.188878     112 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1732457295.389783     112 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81875/81875 [==============================] - 8177s 99ms/step - loss: 1.0461 - val_loss: 0.9587 - accuracy: 0.6533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f1f31f82320>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x=tf_train_set, \n",
    "          validation_data=tf_validation_set, \n",
    "          epochs=num_epochs, \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:23:57.782040Z",
     "iopub.status.busy": "2024-11-24T16:23:57.781470Z",
     "iopub.status.idle": "2024-11-24T16:23:58.576231Z",
     "shell.execute_reply": "2024-11-24T16:23:58.575415Z",
     "shell.execute_reply.started": "2024-11-24T16:23:57.782001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/model/tokenizer_config.json',\n",
       " '/kaggle/working/model/special_tokens_map.json',\n",
       " '/kaggle/working/model/vocab.txt',\n",
       " '/kaggle/working/model/added_tokens.json',\n",
       " '/kaggle/working/model/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"/kaggle/working/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:23:58.577761Z",
     "iopub.status.busy": "2024-11-24T16:23:58.577399Z",
     "iopub.status.idle": "2024-11-24T16:24:21.925533Z",
     "shell.execute_reply": "2024-11-24T16:24:21.924613Z",
     "shell.execute_reply.started": "2024-11-24T16:23:58.577723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4494909d940b47e08812cdb01bba0bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/411972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess test dataset\n",
    "def preprocess_test(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "test_ds = Dataset.from_pandas(raw_test[['text']])\n",
    "tokenized_test_ds = test_ds.map(preprocess_test, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:21.927407Z",
     "iopub.status.busy": "2024-11-24T16:24:21.926762Z",
     "iopub.status.idle": "2024-11-24T16:24:21.986062Z",
     "shell.execute_reply": "2024-11-24T16:24:21.985466Z",
     "shell.execute_reply.started": "2024-11-24T16:24:21.927363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare test set for TensorFlow\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_test_ds,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-24T16:24:21.987235Z",
     "iopub.status.busy": "2024-11-24T16:24:21.987013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1279/25749 [>.............................] - ETA: 12:08"
     ]
    }
   ],
   "source": [
    "# Get the predictions\n",
    "predictions = model.predict(tf_test_set)\n",
    "\n",
    "# Convert logits to predicted class indices\n",
    "predicted_class_indices = np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "# Map class indices to their corresponding labels\n",
    "predicted_labels = [id2label[idx] for idx in predicted_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = raw_test.copy()\n",
    "submission = raw_test[['tweet_id', 'emotion']]\n",
    "submission['emotion'] = predicted_labels\n",
    "submission = submission.rename(columns={'tweet_id': 'id'})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9912598,
     "sourceId": 87232,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 10260365,
     "datasetId": 6087178,
     "sourceId": 9992918,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 10220360,
     "datasetId": 6123966,
     "sourceId": 9957130,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
